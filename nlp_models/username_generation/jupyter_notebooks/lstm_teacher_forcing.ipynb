{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My RNN Language Model implementation in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'*': 0, '<': 1, '>': 2, '\\n': 3, '0': 4, '1': 5, '2': 6, '3': 7, '4': 8, '5': 9, '6': 10, '7': 11, '8': 12, '9': 13, 'a': 14, 'b': 15, 'c': 16, 'd': 17, 'e': 18, 'f': 19, 'g': 20, 'h': 21, 'i': 22, 'j': 23, 'k': 24, 'l': 25, 'm': 26, 'n': 27, 'o': 28, 'p': 29, 'q': 30, 'r': 31, 's': 32, 't': 33, 'u': 34, 'v': 35, 'w': 36, 'x': 37, 'y': 38, 'z': 39}\n",
      "Number of characters: 401244\n",
      "Number of unique characters: 40\n",
      "Longest word: rabbitsreviews\n",
      "Length of longest word: 14\n"
     ]
    }
   ],
   "source": [
    "with open('../datasets/username_dataset.txt', 'r') as file:\n",
    "    data = file.readlines()\n",
    "    \n",
    "data = [example.strip().lower() for example in data]\n",
    "\n",
    "with open('../datasets/username_dataset.txt', 'r') as file:\n",
    "    data_str = file.read().lower()\n",
    "\n",
    "vocab = list(set(data_str))\n",
    "vocab.sort()\n",
    "\n",
    "vocab.insert(0, '>') # End token: <end>\n",
    "vocab.insert(0, '<') # Start token: <start>\n",
    "vocab.insert(0, '*') # Padding\n",
    "\n",
    "char_to_index = {ch: i for i, ch in enumerate(vocab)}\n",
    "index_to_char = {i: ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "print(char_to_index)\n",
    "\n",
    "longest_word = max(data, key=len)\n",
    "\n",
    "print(f\"Number of characters: {len(data_str)}\")\n",
    "print(f\"Number of unique characters: {len(vocab)}\")\n",
    "print(f\"Longest word: {longest_word}\")\n",
    "print(f\"Length of longest word: {len(longest_word)}\") # Needed information to know what length to pad the usernames to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '*' :   0,\n",
      "  '<' :   1,\n",
      "  '>' :   2,\n",
      "  '\\n':   3,\n",
      "  '0' :   4,\n",
      "  '1' :   5,\n",
      "  '2' :   6,\n",
      "  '3' :   7,\n",
      "  '4' :   8,\n",
      "  '5' :   9,\n",
      "  '6' :  10,\n",
      "  '7' :  11,\n",
      "  '8' :  12,\n",
      "  '9' :  13,\n",
      "  'a' :  14,\n",
      "  'b' :  15,\n",
      "  'c' :  16,\n",
      "  'd' :  17,\n",
      "  'e' :  18,\n",
      "  'f' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def print_mapping(mapping):\n",
    "    print('{')\n",
    "    for char, _ in zip(mapping, range(20)):\n",
    "        print('  {:4s}: {:3d},'.format(repr(char), mapping[char]))\n",
    "    print('  ...\\n}')\n",
    "    \n",
    "print_mapping(char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54037, 15, 1)\n",
      "(54037, 15, 1)\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(data, username_length):\n",
    "    \"\"\"Adds <start> and <end> tokens and pads the usernames.\"\"\"\n",
    "    data_X = []\n",
    "    data_Y = []\n",
    "    for username in data:\n",
    "        pad = \"*\" * (username_length - len(username))\n",
    "        X = np.array([char_to_index[char] for char in f\"<{username}{pad}\"]).reshape((username_length + 1, 1))\n",
    "        Y = np.array([char_to_index[char] for char in f\"{username}>{pad}\"]).reshape((username_length + 1, 1))\n",
    "        data_X.append(X)\n",
    "        data_Y.append(Y)\n",
    "    \n",
    "\n",
    "    return (np.array(data_X), np.array(data_Y))\n",
    "\n",
    "data = prepare_dataset(data, len(longest_word))\n",
    "\n",
    "print(data[0].shape)\n",
    "print(data[1].shape)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  '<info**********'\n",
      "Target data: 'info>**********'\n",
      "Input data:  [[ 1]\n",
      " [22]\n",
      " [27]\n",
      " [19]\n",
      " [28]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]]\n",
      "Target data: [[22]\n",
      " [27]\n",
      " [19]\n",
      " [28]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]]\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print('Input data: ', repr(''.join([index_to_char[i] for i in input_example[:, 0].numpy()])))\n",
    "    print('Target data:', repr(''.join([index_to_char[i] for i in target_example[:, 0].numpy()])))\n",
    "    \n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print('Input data: ', input_example.numpy())\n",
    "    print('Target data:', target_example.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((128, 15, 1), (128, 15, 1)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 10000\n",
    "EPOCHS = 100\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "rnn_units = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, rnn_units, batch_size, stateful=False):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Masking(mask_value=0, batch_input_shape=[batch_size, None, 1]),\n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             stateful=stateful,\n",
    "                             recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size, activation=tf.nn.softmax)])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            (128, None, 1)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (128, None, 512)          1052672   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (128, None, 40)           20520     \n",
      "=================================================================\n",
      "Total params: 1,073,192\n",
      "Trainable params: 1,073,192\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                               min_delta=0,\n",
    "                                               patience=5,\n",
    "                                               verbose=0,\n",
    "                                               mode='auto',\n",
    "                                               baseline=None,\n",
    "                                               restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "422/422 [==============================] - 72s 172ms/step - loss: 1.3925\n",
      "Epoch 2/100\n",
      "422/422 [==============================] - 63s 150ms/step - loss: 1.3436\n",
      "Epoch 3/100\n",
      "422/422 [==============================] - 61s 144ms/step - loss: 1.3328\n",
      "Epoch 4/100\n",
      "422/422 [==============================] - 59s 139ms/step - loss: 1.3231\n",
      "Epoch 5/100\n",
      "422/422 [==============================] - 59s 140ms/step - loss: 1.3146\n",
      "Epoch 6/100\n",
      "422/422 [==============================] - 64s 152ms/step - loss: 1.3062\n",
      "Epoch 7/100\n",
      "422/422 [==============================] - 59s 140ms/step - loss: 1.2982\n",
      "Epoch 8/100\n",
      "422/422 [==============================] - 61s 144ms/step - loss: 1.29001s - l\n",
      "Epoch 9/100\n",
      "422/422 [==============================] - 61s 145ms/step - loss: 1.2810\n",
      "Epoch 10/100\n",
      "422/422 [==============================] - 60s 143ms/step - loss: 1.2726\n",
      "Epoch 11/100\n",
      "422/422 [==============================] - 60s 143ms/step - loss: 1.2643\n",
      "Epoch 12/100\n",
      "422/422 [==============================] - 61s 144ms/step - loss: 1.2571\n",
      "Epoch 13/100\n",
      "422/422 [==============================] - 62s 147ms/step - loss: 1.2490\n",
      "Epoch 14/100\n",
      "422/422 [==============================] - 65s 155ms/step - loss: 1.2421\n",
      "Epoch 15/100\n",
      "422/422 [==============================] - 63s 150ms/step - loss: 1.2348\n",
      "Epoch 16/100\n",
      "422/422 [==============================] - 65s 153ms/step - loss: 1.2273\n",
      "Epoch 17/100\n",
      "422/422 [==============================] - 63s 150ms/step - loss: 1.2207\n",
      "Epoch 18/100\n",
      "422/422 [==============================] - 64s 152ms/step - loss: 1.2138\n",
      "Epoch 19/100\n",
      "422/422 [==============================] - 63s 150ms/step - loss: 1.2066\n",
      "Epoch 20/100\n",
      "422/422 [==============================] - 65s 153ms/step - loss: 1.2008\n",
      "Epoch 21/100\n",
      "422/422 [==============================] - 66s 157ms/step - loss: 1.1940\n",
      "Epoch 22/100\n",
      "422/422 [==============================] - 66s 156ms/step - loss: 1.1872\n",
      "Epoch 23/100\n",
      "422/422 [==============================] - 65s 155ms/step - loss: 1.1810\n",
      "Epoch 24/100\n",
      "422/422 [==============================] - 65s 154ms/step - loss: 1.1746\n",
      "Epoch 25/100\n",
      "422/422 [==============================] - 73s 173ms/step - loss: 1.1688\n",
      "Epoch 26/100\n",
      "422/422 [==============================] - 66s 157ms/step - loss: 1.1626\n",
      "Epoch 27/100\n",
      "422/422 [==============================] - 68s 161ms/step - loss: 1.1565\n",
      "Epoch 28/100\n",
      "422/422 [==============================] - 68s 161ms/step - loss: 1.1514\n",
      "Epoch 29/100\n",
      "422/422 [==============================] - 67s 160ms/step - loss: 1.1455\n",
      "Epoch 30/100\n",
      "422/422 [==============================] - 68s 161ms/step - loss: 1.1400\n",
      "Epoch 31/100\n",
      "422/422 [==============================] - 74s 175ms/step - loss: 1.1346\n",
      "Epoch 32/100\n",
      "422/422 [==============================] - 69s 164ms/step - loss: 1.1292\n",
      "Epoch 33/100\n",
      "422/422 [==============================] - 70s 165ms/step - loss: 1.1246\n",
      "Epoch 34/100\n",
      "422/422 [==============================] - 76s 181ms/step - loss: 1.1192\n",
      "Epoch 35/100\n",
      "422/422 [==============================] - 77s 182ms/step - loss: 1.1141\n",
      "Epoch 36/100\n",
      "422/422 [==============================] - 86s 204ms/step - loss: 1.1089\n",
      "Epoch 37/100\n",
      "422/422 [==============================] - 88s 209ms/step - loss: 1.10460s - loss: 1.1\n",
      "Epoch 38/100\n",
      "422/422 [==============================] - 88s 209ms/step - loss: 1.0998\n",
      "Epoch 39/100\n",
      "422/422 [==============================] - 82s 195ms/step - loss: 1.0954\n",
      "Epoch 40/100\n",
      "422/422 [==============================] - 83s 196ms/step - loss: 1.0906\n",
      "Epoch 41/100\n",
      "422/422 [==============================] - 84s 199ms/step - loss: 1.0864\n",
      "Epoch 42/100\n",
      "422/422 [==============================] - 84s 200ms/step - loss: 1.0821\n",
      "Epoch 43/100\n",
      "422/422 [==============================] - 85s 201ms/step - loss: 1.0778\n",
      "Epoch 44/100\n",
      "422/422 [==============================] - 86s 203ms/step - loss: 1.0736\n",
      "Epoch 45/100\n",
      "422/422 [==============================] - 86s 204ms/step - loss: 1.0704\n",
      "Epoch 46/100\n",
      "422/422 [==============================] - 87s 207ms/step - loss: 1.0658\n",
      "Epoch 47/100\n",
      "422/422 [==============================] - 89s 211ms/step - loss: 1.0622\n",
      "Epoch 48/100\n",
      "422/422 [==============================] - 90s 213ms/step - loss: 1.0583\n",
      "Epoch 49/100\n",
      "422/422 [==============================] - 95s 226ms/step - loss: 1.0549\n",
      "Epoch 50/100\n",
      "422/422 [==============================] - 92s 217ms/step - loss: 1.0511\n",
      "Epoch 51/100\n",
      "422/422 [==============================] - 92s 219ms/step - loss: 1.0483\n",
      "Epoch 52/100\n",
      "422/422 [==============================] - 94s 222ms/step - loss: 1.0443\n",
      "Epoch 53/100\n",
      "422/422 [==============================] - 94s 222ms/step - loss: 1.0416\n",
      "Epoch 54/100\n",
      "422/422 [==============================] - 100s 237ms/step - loss: 1.0381\n",
      "Epoch 55/100\n",
      "422/422 [==============================] - 102s 243ms/step - loss: 1.0349\n",
      "Epoch 56/100\n",
      "422/422 [==============================] - 92s 219ms/step - loss: 1.0316\n",
      "Epoch 57/100\n",
      "422/422 [==============================] - 95s 225ms/step - loss: 1.0289\n",
      "Epoch 58/100\n",
      "422/422 [==============================] - 101s 240ms/step - loss: 1.0259\n",
      "Epoch 59/100\n",
      "422/422 [==============================] - 100s 236ms/step - loss: 1.0233\n",
      "Epoch 60/100\n",
      "422/422 [==============================] - 94s 223ms/step - loss: 1.0198\n",
      "Epoch 61/100\n",
      "422/422 [==============================] - 104s 245ms/step - loss: 1.0174\n",
      "Epoch 62/100\n",
      "422/422 [==============================] - 102s 241ms/step - loss: 1.0146\n",
      "Epoch 63/100\n",
      "422/422 [==============================] - 97s 229ms/step - loss: 1.0123\n",
      "Epoch 64/100\n",
      "422/422 [==============================] - 98s 231ms/step - loss: 1.0091\n",
      "Epoch 65/100\n",
      "422/422 [==============================] - 99s 234ms/step - loss: 1.0073\n",
      "Epoch 66/100\n",
      "422/422 [==============================] - 102s 242ms/step - loss: 1.0046\n",
      "Epoch 67/100\n",
      "422/422 [==============================] - 103s 244ms/step - loss: 1.0026\n",
      "Epoch 68/100\n",
      "422/422 [==============================] - 104s 245ms/step - loss: 1.0001\n",
      "Epoch 69/100\n",
      "422/422 [==============================] - 103s 245ms/step - loss: 0.9974\n",
      "Epoch 70/100\n",
      "422/422 [==============================] - 104s 247ms/step - loss: 0.9955\n",
      "Epoch 71/100\n",
      "422/422 [==============================] - 105s 249ms/step - loss: 0.9930\n",
      "Epoch 72/100\n",
      "422/422 [==============================] - 107s 253ms/step - loss: 0.9913\n",
      "Epoch 73/100\n",
      "422/422 [==============================] - 108s 256ms/step - loss: 0.9888\n",
      "Epoch 74/100\n",
      "422/422 [==============================] - 108s 257ms/step - loss: 0.9867\n",
      "Epoch 75/100\n",
      "422/422 [==============================] - 117s 278ms/step - loss: 0.9848\n",
      "Epoch 76/100\n",
      "422/422 [==============================] - 111s 263ms/step - loss: 0.9827\n",
      "Epoch 77/100\n",
      "422/422 [==============================] - 113s 267ms/step - loss: 0.9810\n",
      "Epoch 78/100\n",
      "422/422 [==============================] - 132s 312ms/step - loss: 0.9795\n",
      "Epoch 79/100\n",
      "422/422 [==============================] - 129s 307ms/step - loss: 0.9771\n",
      "Epoch 80/100\n",
      "422/422 [==============================] - 133s 316ms/step - loss: 0.9755\n",
      "Epoch 81/100\n",
      "422/422 [==============================] - 134s 317ms/step - loss: 0.9735\n",
      "Epoch 82/100\n",
      "422/422 [==============================] - 135s 320ms/step - loss: 0.9720\n",
      "Epoch 83/100\n",
      "422/422 [==============================] - 135s 321ms/step - loss: 0.9707\n",
      "Epoch 84/100\n",
      "422/422 [==============================] - 137s 323ms/step - loss: 0.9685\n",
      "Epoch 85/100\n",
      "422/422 [==============================] - 139s 329ms/step - loss: 0.9669\n",
      "Epoch 86/100\n",
      "422/422 [==============================] - 146s 345ms/step - loss: 0.9652\n",
      "Epoch 87/100\n",
      "422/422 [==============================] - 140s 332ms/step - loss: 0.9634\n",
      "Epoch 88/100\n",
      "422/422 [==============================] - 141s 334ms/step - loss: 0.9623\n",
      "Epoch 89/100\n",
      "422/422 [==============================] - 143s 338ms/step - loss: 0.9608\n",
      "Epoch 90/100\n",
      "422/422 [==============================] - 145s 345ms/step - loss: 0.9591\n",
      "Epoch 91/100\n",
      "422/422 [==============================] - 147s 347ms/step - loss: 0.9575\n",
      "Epoch 92/100\n",
      "422/422 [==============================] - 145s 343ms/step - loss: 0.9559\n",
      "Epoch 93/100\n",
      "422/422 [==============================] - 153s 363ms/step - loss: 0.9544\n",
      "Epoch 94/100\n",
      "422/422 [==============================] - 148s 350ms/step - loss: 0.9533\n",
      "Epoch 95/100\n",
      "422/422 [==============================] - 150s 356ms/step - loss: 0.9519\n",
      "Epoch 96/100\n",
      "422/422 [==============================] - 149s 354ms/step - loss: 0.9504\n",
      "Epoch 97/100\n",
      "422/422 [==============================] - 162s 384ms/step - loss: 0.9493\n",
      "Epoch 98/100\n",
      "422/422 [==============================] - 158s 375ms/step - loss: 0.9482\n",
      "Epoch 99/100\n",
      "422/422 [==============================] - 154s 365ms/step - loss: 0.9466\n",
      "Epoch 100/100\n",
      "422/422 [==============================] - 162s 384ms/step - loss: 0.9451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a44e978108>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=EPOCHS, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('../weights/lstm_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, rnn_units, batch_size=1, stateful=True)\n",
    "\n",
    "model.load_weights('../weights/lstm_model_weights.h5')\n",
    "\n",
    "# model.build(tf.TensorShape([1, None, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_usernames(model):\n",
    "    num_generate = 7\n",
    "    generated_usernames = []\n",
    "\n",
    "    for i in range(num_generate):\n",
    "        model.reset_states()\n",
    "        input_eval = np.array([char_to_index['<']]).reshape((1, 1, 1))\n",
    "        generated_username = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            predictions = model.predict(input_eval)\n",
    "            predictions = tf.squeeze(predictions)\n",
    "\n",
    "            predicted_id = np.random.choice(range(40), p=predictions.numpy())\n",
    "            # predicted_id = np.argmax(predictions)\n",
    "\n",
    "            input_eval = np.array([predicted_id]).reshape((1, 1, 1))\n",
    "            \n",
    "            done = index_to_char[predicted_id] in ['>', '*']\n",
    "            \n",
    "            if not done:\n",
    "                generated_username.append(index_to_char[predicted_id])\n",
    "        generated_usernames.append(\"\".join(generated_username))\n",
    "\n",
    "    return generated_usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ventel', 'broeer', 'mushtoop', 'dosk', 'somoco', 'toflanp', 'blacer']\n",
      "['olenic', 'arvisu', 'heklomip', 'tter', 'spnyrginar', 'miqtx', 'kbrbmner']\n",
      "['stretch1', 'bosexavnr', 'moiiem', 'ruggo1', 'sacison', 'cicuinne', 'sieefy']\n",
      "['garthw', 'chaoges', 'gererhf', 'sptics', 'kahal', 'rlxdavg', 'pane1100']\n",
      "['fredlan', 'stdbhthf', 'tuail', 'dinar', 'repley', 'dncrroh', 'seyglwg']\n",
      "['siggen', 'dave69', 'sauan13', 'bearpy', 'trjcian', 'erocerah', 'duckmunt']\n",
      "['argc', 'charliex', 'caryash', 'poinmo', 'tqucky', 'jakesne', 'psinter']\n",
      "['topnap', 'slartupm', 'alland', 'cabl', 'heory123', 'fbonwas', 'jndsth']\n",
      "['anad', 'henry69', 'fodnre', 'mboidy200', 'greyfox', 'pawlgwey', 'maddawg']\n",
      "['retepe', 'petec', 'demlan', 'mhckey', 'gogfa', 'serti', 'eirayusr']\n",
      "['shgnyb', 'saibot', 'swpod', 'mahike', 'nylonjan', 'tiezerk', '12vibh']\n",
      "['oeily', 'mlldy', 'laconi', 'glole', 'jbhrl', 'fvln1133', 'ampter']\n",
      "['bigmac', 'cearar', 'hamster1', 'buddy12', 'brady1', 'optifr1', 'dufavi1']\n",
      "['bigcogc', 'rarta2', 'katmind', 'tulaba', 'dyrorser', 'paurici7', 'sorre7']\n",
      "['dogsnn', 'forng', 'domber', 'cotnt2', 'amlan', 'schafer', 'ducost']\n",
      "['trohg', 'brickwor', 'biszjn', 'evad', 'marcello', 'tprn', 'lmopley']\n",
      "['blue10', 'sajnt89', 'char', 'sexgirl', 'soida1', 'annetms', 'bill11']\n",
      "['moelan', 'emevaber', 'tcatesmo', 'blowm', 'argent1', 'tact', 'bblbwan']\n",
      "['jnhnwziaw', 'adreye', 'bglly', 'feramd', 'petror', 'jssev', 'markjaml']\n",
      "['cwarren', 'thomeel', 'ashfnom', 'duqce', 'sexming', 'cwbrre', 'terry123']\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    generated_usernames = generate_usernames(model)\n",
    "    print(generated_usernames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
